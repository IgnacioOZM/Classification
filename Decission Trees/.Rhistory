#TEST
#Overall accuracy
confusionMatrix(fTS_eval$LRpred, fTS_eval$Y, positive = "YES")$overall[1]
confusionMatrix(fTS_eval$LRpred2, fTS_eval$Y, positive = "YES")$overall[1]
confusionMatrix(fTS_eval$knn_pred, fTS_eval$Y, positive = "YES")$overall[1]
#Calibration curve
calPlotData <- calibration(Y ~ LRprob$YES+LRprob2$YES+knn_prob$YES, data =fTS_eval, class = "YES",cuts = 6)
xyplot(calPlotData, auto.key = list(columns = 2))
#ROC curve
library(pROC)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$LRprob$YES)
plot(reducedRoc, col="black")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$LRprob2$YES)
plot(reducedRoc, add=TRUE, col="red")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$knn_prob$YES)
plot(reducedRoc, add=TRUE, col="green")
auc(reducedRoc)
legend("bottomright", legend=c("LR", "LR2","knn"), col=c("black", "red","green"), lwd=2)
source('C:/Users/Ignacio/OneDrive - Universidad Pontificia Comillas/ICAI/1ยบ de Master/Machine learning/Practica_2_2_Classification/Lab_2_2_DT_RF.R', echo=TRUE)
## Exploratory analysis -------------------------------------------------------------------------------------
ggplot(fdata) + geom_point(aes(x = X1, y = X2, color = Y))
## Divide the data into training and test sets ---------------------------------------------------
set.seed(150) #For replication
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$Y,      #output variable. createDataPartition creates proportional partitions
p = 0.8,      #split probability for training
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
## Exploratory analysis -------------------------------------------------------------------------------------
ggplot(fdata) + geom_point(aes(x = X1, y = X2, color = Y))
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$Y,      #output variable. createDataPartition creates proportional partitions
p = 0.8,      #split probability for training
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
#obtain training and test sets
fTR <- fdata[trainIndex,]
fTS <- fdata[-trainIndex,]
#Create datasets to store evaluations
fTR_eval <- fTR
fTS_eval <- fTS
## Initialize trainControl -----------------------------------------------------------------------
ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
#-------------------------------------------------------------------------------------------------
#---------------------------- DECISION TREE ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(rpart)
library(rpart.plot)
library(partykit)
set.seed(150) #For replication
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,c(1,2)],  #Input variables.
y = fTR$Y,   #Output variable
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 5), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.025), # TRY this: tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
tuneGrid = data.frame(cp = seq(0,0.05,0.0005)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
summary(tree.fit)  #information about the model trained
tree.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Basic plot of the tree:
plot(tree.fit$finalModel, uniform = TRUE, margin = 0)
text(tree.fit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
#Advanced plots
rpart.plot(tree.fit$finalModel, type = 2, fallen.leaves = FALSE, box.palette = "Oranges")
tree.fit.party <- as.party(tree.fit$finalModel)
plot(tree.fit.party)
#Measure for variable importance
varImp(tree.fit,scale = FALSE)
plot(varImp(tree.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and test sets
#training
fTR_eval$tree_prob <- predict(tree.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$tree_pred <- predict(tree.fit, type="raw", newdata = fTR) # predict classes
#test
fTS_eval$tree_prob <- predict(tree.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$tree_pred <- predict(tree.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
tree.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
# test
confusionMatrix(fTS_eval$tree_pred,
fTS_eval$Y,
positive = "YES")
#######Classification performance plots
# Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
fTR_eval$tree_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed
# test
PlotClassPerformance(fTS_eval$Y,       #Real observations
fTS_eval$tree_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed)
#-------------------------------------------------------------------------------------------------
#---------------------------- RANDOM FOREST ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(randomForest)
set.seed(150) #For replication
#Train decision tree
#rf contains one tuning parameter mtry:
#   the number of variables randomly sampled as candidates at each split.
#   The ntree argument can be used to specify the number of trees to grow.
rf.fit <- train(  x = fTR[,1:2],   #Input variables
y = fTR$Y,   #Output variables
method = "rf", #Random forest
ntree = 200,  #Number of trees to grow
tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)),
tuneLength = 4,
trControl = ctrl, #Resampling settings
metric = "Accuracy") #Summary metrics
rf.fit #information about the resampling settings
ggplot(rf.fit)
#Train decision tree
#rf contains one tuning parameter mtry:
#   the number of variables randomly sampled as candidates at each split.
#   The ntree argument can be used to specify the number of trees to grow.
rf.fit <- train(  x = fTR[,1:2],   #Input variables
y = fTR$Y,   #Output variables
method = "rf", #Random forest
ntree = 200,  #Number of trees to grow
tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)),
tuneLength = 4,
trControl = ctrl, #Resampling settings
metric = "Accuracy") #Summary metrics
rf.fit #information about the resampling settings
ggplot(rf.fit)
set.seed(150) #For replication
#Train decision tree
#rf contains one tuning parameter mtry:
#   the number of variables randomly sampled as candidates at each split.
#   The ntree argument can be used to specify the number of trees to grow.
rf.fit <- train(  x = fTR[,1:2],   #Input variables
y = fTR$Y,   #Output variables
method = "rf", #Random forest
ntree = 200,  #Number of trees to grow
tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)),
tuneLength = 4,
trControl = ctrl, #Resampling settings
metric = "Accuracy") #Summary metrics
rf.fit #information about the resampling settings
ggplot(rf.fit)
set.seed(150) #For replication
#Train decision tree
#rf contains one tuning parameter mtry:
#   the number of variables randomly sampled as candidates at each split.
#   The ntree argument can be used to specify the number of trees to grow.
rf.fit <- train(  x = fTR[,1:2],   #Input variables
y = fTR$Y,   #Output variables
method = "rf", #Random forest
ntree = 2000,  #Number of trees to grow
tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)),
tuneLength = 4,
trControl = ctrl, #Resampling settings
metric = "Accuracy") #Summary metrics
rf.fit #information about the resampling settings
ggplot(rf.fit)
#Measure for variable importance
varImp(rf.fit,scale = FALSE)
plot(varImp(rf.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#training
fTR_eval$rf_prob <- predict(rf.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$rf_pred <- predict(rf.fit, type="raw", newdata = fTR) # predict classes
#Test
fTS_eval$rf_prob <- predict(rf.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$rf_pred <- predict(rf.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
rf.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$rf_pred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
# Validation
confusionMatrix(fTS_eval$rf_pred,
fTS_eval$Y,
positive = "YES")
#######Classification performance plots
# Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
fTR_eval$rf_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed
# Validation
PlotClassPerformance(fTS_eval$Y,       #Real observations
fTS_eval$rf_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed)
library(xgboost)
set.seed(150) #For replication
#Parameter tuning
xgb_grid <- expand.grid(
nrounds = 150, #Boosting Iterations
eta = 0.3,     #Shrinkage
max_depth = 5, #Max Tree Depth
gamma = 0,    #Minimum Loss Reduction
colsample_bytree=1, #Subsample Ratio of Columns
min_child_weight=1, #Minimum Sum of Instance Weight
subsample = 0.5    #Subsample Percentage
)
# train
xgb.fit = train(
x = fTR[,1:2],   #Input variables
y = fTR$Y,   #Output variables
#tuneGrid = xgb_grid, #Uncomment to use values previously defined
tuneLength = 4, #Use caret tuning
method = "xgbTree",
trControl = ctrl,
metric="Accuracy"
)
tuneplot(xgb.fit)
#plot grid
# helper function for the plots
tuneplot <- function(x, probs = .90) {
ggplot(x) +
coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
theme_bw()
}
#plot grid
# helper function for the plots
tuneplot <- function(x, probs = .90) {
ggplot(x) +
coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
theme_bw()
}
tuneplot(xgb.fit)
#plot grid
# helper function for the plots
tuneplot <- function(x, probs = .90) {
ggplot(x) +
coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
theme_bw()
}
tuneplot(xgb.fit)
#Measure for variable importance
varImp(xgb.fit,scale = FALSE)
plot(varImp(xgb.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#training
fTR_eval$xgb_prob <- predict(xgb.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$xgb_pred <- predict(xgb.fit, type="raw", newdata = fTR) # predict classes
#Test
fTS_eval$xgb_prob <- predict(xgb.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$xgb_pred <- predict(xgb.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
xgb.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$xgb_pred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
# Validation
confusionMatrix(fTS_eval$xgb_pred,
fTS_eval$Y,
positive = "YES")
#######Classification performance plots
# Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
fTR_eval$xgb_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed
# Validation
PlotClassPerformance(fTS_eval$Y,       #Real observations
fTS_eval$xgb_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed)
summary(transformResults)
## comparison of models in training and validation set --------------------------------------------------------
#resampling summary metric
transformResults <- resamples(list(tree = tree.fit, rf = rf.fit, xgb = xgb.fit))
dotplot(transformResults)
source('C:/Users/Ignacio/OneDrive - Universidad Pontificia Comillas/ICAI/1ยบ de Master/Machine learning/Practica_2_3_Classification_SVM/LabPractice_2_3_SVM.R', echo=TRUE)
#-------------------------------------------------------------------------------------------------
#---------------------------- SVM LINEAR ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(kernlab)
set.seed(150) #For replication
#Train linear  svm
#svm contains 1 tuning parameter C (Cost). Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame(C = 0.1),
#  - Try with a range of values specified in tuneGrid: tuneGrid = data.frame(cp = seq(0.1,10,0.5)),
#  - Caret chooses 10 values: tuneLength = 10,
svm.fit <- train(form = Y ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "svmLinear",
preProcess = c("center","scale"),
# 1) try C=0.1
tuneGrid = data.frame(C = 0.1),
# 2) try C=10 and compare with C=0.1
#tuneGrid = data.frame(C = 10),
# 3) find the optimal value of C
#tuneGrid = expand.grid(C = c(0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000)),
#tuneGrid = data.frame(C = seq(0.1,10,1)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
svm.fit #information about the resampling settings
ggplot(svm.fit) + scale_x_log10()
svm.fit$finalModel #information about the model trained
#Plot the svm support vectors:
isupvect <- alphaindex(svm.fit$finalModel)[[1]] #indexes for support vectors
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,],aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and test sets
#training
fTR_eval <- fTR
fTR_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTR) # predict classes
#test
fTS_eval <- fTS
fTS_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
svm.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
set.seed(150) #For replication
svm.fit #information about the resampling settings
#-------------------------------------------------------------------------------------------------
#---------------------------- SVM RADIAL ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(kernlab)
#Train model using training data
#Train radial  svm
#svm contains 2 tuning parameter C (Cost) and sigma. Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame( sigma=100, C=1),
#  - Try with a range of values specified in tuneGrid: tuneGrid = expand.grid(C = seq(0.1,100,length.out = 8), sigma=seq(0.01,50,length.out = 4)),
#  - Caret chooses 10 values: tuneLength = 10,
svm.fit = train(form = Y ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "svmRadial",
preProcess = c("center","scale"),
tuneGrid = expand.grid(C = c(0.001,0.01,0.1,1,10,100,1000), sigma=c(0.0001,0.001,0.01,0.1,1,10)),
#tuneGrid =  data.frame(sigma = 0.01, C = 0.1),
#tuneGrid = expand.grid(C = seq(0.1,1000,length.out = 8), sigma=seq(0.01,50,length.out = 4)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
ggplot(svm.fit) + scale_x_log10()
ggplot(svm.fit) + scale_x_log10()
set.seed(150) #For replication
#Train linear  svm
#svm contains 1 tuning parameter C (Cost). Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame(C = 0.1),
#  - Try with a range of values specified in tuneGrid: tuneGrid = data.frame(cp = seq(0.1,10,0.5)),
#  - Caret chooses 10 values: tuneLength = 10,
svm.fit <- train(form = Y ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "svmLinear",
preProcess = c("center","scale"),
# 1) try C=0.1
# tuneGrid = data.frame(C = 0.1),
# 2) try C=10 and compare with C=0.1
#tuneGrid = data.frame(C = 10),
# 3) find the optimal value of C
tuneGrid = expand.grid(C = c(0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000)),
#tuneGrid = data.frame(C = seq(0.1,10,1)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
svm.fit #information about the resampling settings
ggplot(svm.fit) + scale_x_log10()
svm.fit$finalModel #information about the model trained
set.seed(150) #For replication
#Train linear  svm
#svm contains 1 tuning parameter C (Cost). Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame(C = 0.1),
#  - Try with a range of values specified in tuneGrid: tuneGrid = data.frame(cp = seq(0.1,10,0.5)),
#  - Caret chooses 10 values: tuneLength = 10,
svm.fit <- train(form = Y ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "svmLinear",
preProcess = c("center","scale"),
# 1) try C=0.1
# tuneGrid = data.frame(C = 0.1),
# 2) try C=10 and compare with C=0.1
#tuneGrid = data.frame(C = 10),
# 3) find the optimal value of C
#tuneGrid = expand.grid(C = c(0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000)),
tuneGrid = data.frame(C = seq(0.1,10,1)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
svm.fit #information about the resampling settings
ggplot(svm.fit) + scale_x_log10()
svm.fit$finalModel #information about the model trained
#Plot the svm support vectors:
isupvect <- alphaindex(svm.fit$finalModel)[[1]] #indexes for support vectors
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,],aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,], aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
plot(svm.fit$finalModel, data = as.matrix(predict(svm.fit$preProcess,fTR[,1:2])))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and test sets
#training
fTR_eval <- fTR
fTR_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTR) # predict classes
#test
fTS_eval <- fTS
fTS_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
svm.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#-------------------------------------------------------------------------------------------------
#---------------------------- SVM RADIAL ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(kernlab)
set.seed(150) #For replication
#Train model using training data
#Train radial  svm
#svm contains 2 tuning parameter C (Cost) and sigma. Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame( sigma=100, C=1),
#  - Try with a range of values specified in tuneGrid: tuneGrid = expand.grid(C = seq(0.1,100,length.out = 8), sigma=seq(0.01,50,length.out = 4)),
#  - Caret chooses 10 values: tuneLength = 10,
svm.fit = train(form = Y ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "svmRadial",
preProcess = c("center","scale"),
tuneGrid = expand.grid(C = c(0.001,0.01,0.1,1,10,100,1000), sigma=c(0.0001,0.001,0.01,0.1,1,10)),
#tuneGrid =  data.frame(sigma = 0.01, C = 0.1),
#tuneGrid = expand.grid(C = seq(0.1,1000,length.out = 8), sigma=seq(0.01,50,length.out = 4)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
svm.fit #information about the resampling settings
ggplot(svm.fit) + scale_x_log10()
svm.fit$finalModel #information about the model trained
#Plot the svm support vectors:
isupvect <- alphaindex(svm.fit$finalModel)[[1]] #indexes for support vectors
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,], aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
plot(svm.fit$finalModel, data = as.matrix(predict(svm.fit$preProcess,fTR[,1:2])))
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,], aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
plot(svm.fit$finalModel, data = as.matrix(predict(svm.fit$preProcess,fTR[,1:2])))
#plot support vectors
ggplot() + geom_point(data = fTR[isupvect,], aes(x = X1, y = X2), color = "red") +
geom_point(data = fTR[-isupvect,], aes(x = X1, y = X2))
plot(svm.fit$finalModel, data = as.matrix(predict(svm.fit$preProcess,fTR[,1:2])))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and test sets
#training
fTR_eval <- fTR
fTR_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTR) # predict classes
#test
fTS_eval <- fTS
fTS_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTS) # predict classes
#Plot classification in a 2 dimensional space
Plot2DClass(fTR[,1:2], #Input variables of the model
fTR$Y,     #Output variable
svm.fit,#Fitted model with caret
var1 = "X1", var2 = "X2", #variables that define x and y axis
selClass = "YES")     #Class output to be analyzed
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$svm_pred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
# test
confusionMatrix(fTS_eval$svm_pred,
fTS_eval$Y,
positive = "YES")
#######Classification performance plots
# Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
fTR_eval$svm_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed
# test
PlotClassPerformance(fTS_eval$Y,       #Real observations
fTS_eval$svm_prob,  #predicted probabilities
selClass = "YES") #Class to be analyzed)
